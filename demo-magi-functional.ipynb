{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nuke button","metadata":{}},{"cell_type":"code","source":"# Delete all files and folders in the working directory. Use with caution!\nimport shutil\nshutil.rmtree(\"/kaggle/working\", ignore_errors=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:27:36.970562Z","iopub.execute_input":"2025-01-01T19:27:36.971248Z","iopub.status.idle":"2025-01-01T19:27:37.384294Z","shell.execute_reply.started":"2025-01-01T19:27:36.971212Z","shell.execute_reply":"2025-01-01T19:27:37.383085Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Clone repo","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b gender_detection_fix https://github.com/BinhPQ2/magi_functional","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:33:56.539847Z","iopub.execute_input":"2025-01-01T19:33:56.540140Z","iopub.status.idle":"2025-01-01T19:34:04.949828Z","shell.execute_reply.started":"2025-01-01T19:33:56.540109Z","shell.execute_reply":"2025-01-01T19:34:04.949003Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'magi_functional'...\nremote: Enumerating objects: 1966, done.\u001b[K\nremote: Counting objects: 100% (156/156), done.\u001b[K\nremote: Compressing objects: 100% (115/115), done.\u001b[K\nremote: Total 1966 (delta 76), reused 100 (delta 38), pack-reused 1810 (from 1)\u001b[K\nReceiving objects: 100% (1966/1966), 249.12 MiB | 47.56 MiB/s, done.\nResolving deltas: 100% (350/350), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Initialize path","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/magi_functional\n\n# raw_image_path = \"input/raw\"\n# character_path = \"input/character\"\n# voice_bank_path = \"input/voice_bank\"\n\n# raw_image_rename_path = \"output/renamed\"\n# colorized_path = \"output/colorized\"\n# json_path = \"output/json\"\n# transcript_path = \"output/transcript\"\n# audio_path = \"output/audio\"\n# final_output_path =\"output/output_final\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:34:26.714745Z","iopub.execute_input":"2025-01-01T19:34:26.715140Z","iopub.status.idle":"2025-01-01T19:34:26.722713Z","shell.execute_reply.started":"2025-01-01T19:34:26.715107Z","shell.execute_reply":"2025-01-01T19:34:26.721837Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/magi_functional\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Install library","metadata":{}},{"cell_type":"code","source":"cat /kaggle/working/magi_functional/requirements_kaggle.txt | xargs -n 1 pip install -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:35:28.731656Z","iopub.execute_input":"2025-01-01T19:35:28.732003Z","iopub.status.idle":"2025-01-01T19:37:05.637882Z","shell.execute_reply.started":"2025-01-01T19:35:28.731971Z","shell.execute_reply":"2025-01-01T19:37:05.636988Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncartopy 0.24.1 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nalbucore 0.0.16 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nalbumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\narviz 0.19.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nastropy 6.1.3 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nbayesian-optimization 2.0.1 requires numpy>=1.25, but you have numpy 1.22.0 which is incompatible.\nbigframes 1.17.0 requires numpy>=1.24.0, but you have numpy 1.22.0 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\ncudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\ncudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ndipy 1.10.0 requires numpy>=1.22.4, but you have numpy 1.22.0 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.1.4, but you have pandas 1.5.3 which is incompatible.\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\nmizani 0.11.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nmizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\nmne 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\nnumexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.22.0 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npywavelets 1.8.0 requires numpy<3,>=1.23, but you have numpy 1.22.0 which is incompatible.\nrmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\nscikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nstatsmodels 0.14.3 requires numpy<3,>=1.22.3, but you have numpy 1.22.0 which is incompatible.\ntensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\nvisions 0.7.6 requires numpy>=1.23.2, but you have numpy 1.22.0 which is incompatible.\nvisions 0.7.6 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.22.0 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\nxarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\nxarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\nmizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.7.6 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nxarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\ntts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.2.2 which is incompatible.\ncudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nfrom transformers import AutoModel\nimport torch\nimport os\nimport random\nimport re\nimport shutil\nimport argparse\nimport torch\nfrom TTS.api import TTS\nfrom unittest.mock import patch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:37:38.061364Z","iopub.execute_input":"2025-01-01T19:37:38.061686Z","iopub.status.idle":"2025-01-01T19:37:55.060097Z","shell.execute_reply.started":"2025-01-01T19:37:38.061661Z","shell.execute_reply":"2025-01-01T19:37:55.059158Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Check GPU and set device","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\nif torch.cuda.is_available():\n    !nvidia-smi\n    !nvcc --version\nelse:\n    print(\"GPU is not available\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-01T19:37:57.656831Z","iopub.execute_input":"2025-01-01T19:37:57.657130Z","iopub.status.idle":"2025-01-01T19:37:57.966392Z","shell.execute_reply.started":"2025-01-01T19:37:57.657107Z","shell.execute_reply":"2025-01-01T19:37:57.965511Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Wed Jan  1 19:37:57 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             26W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\nRunning on cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Initialize model","metadata":{}},{"cell_type":"markdown","source":"## Download MAGI model","metadata":{}},{"cell_type":"code","source":"magiv2 = AutoModel.from_pretrained(\"ragavsachdeva/magiv2\", trust_remote_code=True).to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:10:09.631988Z","iopub.execute_input":"2024-11-06T18:10:09.633204Z","iopub.status.idle":"2024-11-06T18:11:27.076099Z","shell.execute_reply.started":"2024-11-06T18:10:09.633159Z","shell.execute_reply":"2024-11-06T18:11:27.075081Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/13.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e70250beb448eaa328e5e5f866f610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_magiv2.py:   0%|          | 0.00/1.70k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba100957c0241cc8315145ab9cd4ed8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- configuration_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modelling_magiv2.py:   0%|          | 0.00/34.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e12a726a9245ec9188c1e0241f77b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"utils.py:   0%|          | 0.00/16.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723b8cac254f4bbbbad877492a542767"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processing_magiv2.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203b56455912450cb796ba04ee095c59"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- processing_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/ragavsachdeva/magiv2:\n- modelling_magiv2.py\n- utils.py\n- processing_magiv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac099a93242426188ae3314555296c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee51423d5f534419a0d0bf8f8be32a3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4882bcf1af944ba9ae6fe120e098e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c36d096167b412bacdea6a44536f6e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebe5a36ac0f48c7881620dac161f279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4d3ba4c6454e928b22f78ef577421d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39eb0178f8164ead98f4e7dd180998cb"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Download weight for Text-to-Speech model","metadata":{}},{"cell_type":"code","source":"# Mock input to automatically respond with 'y'\nwith patch('builtins.input', return_value='y'):\n    tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)","metadata":{"execution":{"iopub.status.busy":"2025-01-01T19:38:04.138023Z","iopub.execute_input":"2025-01-01T19:38:04.138330Z","iopub.status.idle":"2025-01-01T19:39:11.947263Z","shell.execute_reply.started":"2025-01-01T19:38:04.138307Z","shell.execute_reply":"2025-01-01T19:39:11.946561Z"},"trusted":true},"outputs":[{"name":"stdout","text":" > You must confirm the following:\n | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.87G/1.87G [00:43<00:00, 42.4MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:44<00:00, 42.2MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.37k/4.37k [00:00<00:00, 19.8kiB/s]\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 361k/361k [00:00<00:00, 1.56MiB/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.0/32.0 [00:00<00:00, 102iB/s]\n 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.00M/7.75M [00:00<00:00, 49.0MiB/s]","output_type":"stream"},{"name":"stdout","text":" > Model's license - CPML\n > Check https://coqui.ai/cpml.txt for more info.\n > Using model: xtts\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.75M/7.75M [00:15<00:00, 49.0MiB/s]/usr/local/lib/python3.10/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Magiv2","metadata":{}},{"cell_type":"code","source":"# Test module magiv2 (worked)\n!python /kaggle/working/magi_functional/magiv2.py --image {raw_image_path} --rename_image {raw_image_rename_path} --character {character_path} --json {json_path} --transcript {transcript_path}","metadata":{"execution":{"iopub.status.busy":"2024-11-06T18:12:21.452413Z","iopub.execute_input":"2024-11-06T18:12:21.453339Z","iopub.status.idle":"2024-11-06T18:12:49.103028Z","shell.execute_reply.started":"2024-11-06T18:12:21.453284Z","shell.execute_reply":"2024-11-06T18:12:49.101733Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on cuda\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.71s/it]\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /opt/conda/lib/python3.10/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/7a6c3a1eb7114afc88f91e0d135d1d7a-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/7a6c3a1eb7114afc88f91e0d135d1d7a-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 496 COLUMNS\nAt line 2087 RHS\nAt line 2579 BOUNDS\nAt line 2740 ENDATA\nProblem MODEL has 491 rows, 160 columns and 1110 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nContinuous objective value is 8.59924 - 0.00 seconds\nCgl0003I 0 fixed, 0 tightened bounds, 27 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 27 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 18 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 9 strengthened rows, 0 substitutions\nCgl0003I 0 fixed, 0 tightened bounds, 9 strengthened rows, 0 substitutions\nCgl0005I 4 SOS with 40 members\nCgl0004I processed model has 13 rows, 40 columns (40 integer (40 of which binary)) and 76 elements\nCbc0038I Initial state - 0 integers unsatisfied sum - 0\nCbc0038I Solution found of 8.59924\nCbc0038I Before mini branch and bound, 40 integers at bound fixed and 0 continuous\nCbc0038I Mini branch and bound did not improve solution (0.01 seconds)\nCbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 8.59924 - took 0.00 seconds\nCbc0012I Integer solution of 8.5992398 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\nCbc0001I Search completed - best objective 8.599239811676402, took 0 iterations and 0 nodes (0.01 seconds)\nCbc0035I Maximum depth 0, 0 variables fixed on reduced cost\nCuts at root node changed objective from 8.59924 to 8.59924\nProbing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nGomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nKnapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nClique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nMixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nFlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nTwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\nZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n\nResult - Optimal solution found\n\nObjective value:                8.59923981\nEnumerated nodes:               0\nTotal iterations:               0\nTime (CPU seconds):             0.01\nTime (Wallclock seconds):       0.01\n\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.97s/it]\n\n\nDone you WEEEEB!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Text to Speech","metadata":{}},{"cell_type":"code","source":"raw_image_rename_path = \"input/raw\"\ntranscript_path = \"input/transcript\"\nvoice_bank_path = \"input/voice_bank\"\noutput_path = \"output\"\ntranscript_file = f\"{transcript_path}/transcript.txt\"","metadata":{"execution":{"iopub.status.busy":"2025-01-01T20:29:22.785611Z","iopub.execute_input":"2025-01-01T20:29:22.785938Z","iopub.status.idle":"2025-01-01T20:29:22.789966Z","shell.execute_reply.started":"2025-01-01T20:29:22.785908Z","shell.execute_reply":"2025-01-01T20:29:22.789137Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!python /kaggle/working/magi_functional/text_to_speech.py -i {raw_image_rename_path} -v {voice_bank_path} -t {transcript_file} -o {output_path}","metadata":{"execution":{"iopub.status.busy":"2025-01-01T20:45:56.655857Z","iopub.execute_input":"2025-01-01T20:45:56.656177Z","iopub.status.idle":"2025-01-01T20:46:39.985812Z","shell.execute_reply.started":"2025-01-01T20:45:56.656146Z","shell.execute_reply":"2025-01-01T20:46:39.984788Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on cuda\n > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n > Using model: xtts\n/usr/local/lib/python3.10/dist-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.speakers = torch.load(speaker_file_path)\n/usr/local/lib/python3.10/dist-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nProcessing character 'other' with text: Eh...?\n > Text splitted to sentences.\n['Eh...?']\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n > Processing time: 2.45693302154541\n > Real-time factor: 1.322124490557309\nProcessing character 'other' with text: All of a sudden\n > Text splitted to sentences.\n['All of a sudden']\n > Processing time: 0.7640430927276611\n > Real-time factor: 0.30318079100642326\nProcessing character 'ruri_female' with text: I grew horns\n > Text splitted to sentences.\n['I grew horns']\n > Processing time: 0.617952823638916\n > Real-time factor: 0.3075537143652514\nProcessing character 'ruri_female' with text: Am I really human?\n > Text splitted to sentences.\n['Am I really human?']\n > Processing time: 0.6500205993652344\n > Real-time factor: 0.299301583194191\nProcessing character 'ruri_female' with text: mom_female, about this...\n > Text splitted to sentences.\n['mom_female, about this...']\n > Processing time: 0.9947559833526611\n > Real-time factor: 0.3232487832015766\nProcessing character 'ruri_female' with text: Oh, Mor- Ning Honey.\n > Text splitted to sentences.\n['Oh, Mor- Ning Honey.']\n > Processing time: 0.8399004936218262\n > Real-time factor: 0.3090755321155084\nProcessing character 'ruri_female' with text: This thing on my head ...\n > Text splitted to sentences.\n['This thing on my head ...']\n > Processing time: 0.5678741931915283\n > Real-time factor: 0.296327763154894\nProcessing character 'ruri_female' with text: It's horns.\n > Text splitted to sentences.\n[\"It's horns.\"]\n > Processing time: 0.4260568618774414\n > Real-time factor: 0.2821526250720081\nProcessing character 'ruri_female' with text: You know this?\n > Text splitted to sentences.\n['You know this?']\n > Processing time: 0.46884655952453613\n > Real-time factor: 0.290395130267304\nProcessing character 'mom_male' with text: But why did they...\n > Text splitted to sentences.\n['But why did they...']\n > Processing time: 0.7322828769683838\n > Real-time factor: 0.3090895374646413\nProcessing character 'mom_male' with text: Ah, that's because you're not a human.\n > Text splitted to sentences.\n[\"Ah, that's because you're not a human.\"]\n > Processing time: 1.1073963642120361\n > Real-time factor: 0.30962276616549245\nProcessing character 'mom_male' with text: Since your Dad\n > Text splitted to sentences.\n['Since your Dad']\n > Processing time: 0.4599337577819824\n > Real-time factor: 0.2954993985749625\nProcessing character 'mom_male' with text: Is a Dragon\n > Text splitted to sentences.\n['Is a Dragon']\n > Processing time: 0.4443488121032715\n > Real-time factor: 0.2942663174818938\nProcessing character 'mom_male' with text: According to your Dad, your body will start to change when you're 15.\n > Text splitted to sentences.\n[\"According to your Dad, your body will start to change when you're 15.\"]\n > Processing time: 1.5502793788909912\n > Real-time factor: 0.32326808428418025\nProcessing character 'mom_male' with text: I didn't expect the horns to grow overnight though.\n > Text splitted to sentences.\n[\"I didn't expect the horns to grow overnight though.\"]\n > Processing time: 1.0453016757965088\n > Real-time factor: 0.3136417095486749\nProcessing character 'mom_male' with text: Anyways, eat your breakfast.\n > Text splitted to sentences.\n['Anyways, eat your breakfast.']\n > Processing time: 0.7844953536987305\n > Real-time factor: 0.30566374309190353\nAudio files have been saved to output\n\u001b[0m","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Combine to video","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/magi_functional/main.py -i {colorized_path} -j {json_path} -a {audio_path} -s {final_output_path} ","metadata":{"execution":{"iopub.status.busy":"2025-01-01T20:20:43.746379Z","iopub.execute_input":"2025-01-01T20:20:43.746692Z","iopub.status.idle":"2025-01-01T20:20:43.994414Z","shell.execute_reply.started":"2025-01-01T20:20:43.746667Z","shell.execute_reply":"2025-01-01T20:20:43.993586Z"},"trusted":true},"outputs":[{"name":"stdout","text":"python3: can't open file '/kaggle/working/manga_read_along/magi_functional/main.py': [Errno 2] No such file or directory\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Download contents","metadata":{}},{"cell_type":"markdown","source":"## Download separate files","metadata":{}},{"cell_type":"markdown","source":"### Download json","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"json_results\", f\"{json_output_dir}/*.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download image results","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"image_results\", result_image_output_dir)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download Audio","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"audio_results\", \"/kaggle/working/manga_read_along/test/audio_output\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/audio_results.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download transcript","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file(\"transcript\", \"/kaggle/working/result/transcript.txt\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download transcript","metadata":{}},{"cell_type":"code","source":"!cd /kaggle/working/\n\nimport os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"male\", \"/kaggle/working/manga_read_along/input/voice_bank/male\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:56:08.661589Z","iopub.execute_input":"2024-11-13T14:56:08.662472Z","iopub.status.idle":"2024-11-13T14:56:09.952291Z","shell.execute_reply.started":"2024-11-13T14:56:08.662425Z","shell.execute_reply":"2024-11-13T14:56:09.951062Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/male.zip","text/html":"<a href='male.zip' target='_blank'>male.zip</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!cd /kaggle/working/\n\nimport os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -rj {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file(\"female\", \"/kaggle/working/manga_read_along/input/voice_bank/female\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T14:57:04.118730Z","iopub.execute_input":"2024-11-13T14:57:04.119327Z","iopub.status.idle":"2024-11-13T14:57:05.384183Z","shell.execute_reply.started":"2024-11-13T14:57:04.119283Z","shell.execute_reply":"2024-11-13T14:57:05.382908Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/female.zip","text/html":"<a href='female.zip' target='_blank'>female.zip</a><br>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Download all","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(download_file_name, source_path):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip -r {zip_name} {source_path}\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n    \n\n!cp -r result_dir /\ndownload_file(\"result\", result_dir)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Edit kaggle","metadata":{}},{"cell_type":"code","source":"process_file = '''\nimport os\nimport random\nimport re\nimport shutil\nimport argparse\nimport torch\nfrom TTS.api import TTS\nfrom unittest.mock import patch\nimport re\nfrom utils.utils import create_save_folder, generate_name_format, get_digit_number_for_name_format\n\n# Function to parse the transcript from a text file\ndef parse_transcript(transcript_path):\n    pages = []\n    current_page = None\n\n    with open(transcript_path, 'r') as file:\n        content = file.readlines()\n    \n    for line in content:\n        if line.startswith(\"<page>\"):\n            # Start a new page\n            if current_page is not None:\n                pages.append(current_page)\n            current_page = {\"page\": re.search(r'<page>(\\d+)<endpage>', line).group(1), \"lines\": []}   \n        elif line.startswith(\"<name>\") and current_page is not None:\n            # Extract character name and dialogue\n            match = re.match(r\"<name>([^<]+)<endname>:\\s*(.+)\", line)\n            if match:\n                character = match.group(1).lower()  # Lowercase for consistency\n                dialogue = match.group(2)\n                current_page[\"lines\"].append((character, dialogue))\n\n    # Add the last page if it exists\n    if current_page is not None:\n        pages.append(current_page)\n        \n    return pages\n\n# Function to get all voice files from the specified directory\ndef get_voice_files(directory):\n    all_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".wav\"):\n                all_files.append(os.path.join(root, file))\n    return all_files\n\n\n# Function to randomly select voice files for characters\ndef select_voice_files_for_characters(characters, voice_bank):\n    all_files = get_voice_files(voice_bank)\n\n    selected_files = {}\n    used_files = set()  # To keep track of used voice files\n\n    for character in characters:\n        gender = character.rsplit('_', 1)[1] if '_' in character else \"unknown\"\n        if gender == \"male\" or gender == \"female\" :\n            # Select a random male voice that hasn't been used\n            gender_files = get_voice_files(os.path.join(voice_bank, gender))\n            available_files = list(set(gender_files) - used_files)\n            if available_files:\n                selected_files[character] = random.choice(available_files)\n                used_files.add(selected_files[character])\n            else:\n                # If rant out of voice, just reuse old one\n                selected_files[character] = random.choice(gender_files)\n        else:\n            # Select any remaining file for other characters, ensuring it's not already used\n            available_files = list(set(all_files) - used_files)\n            if available_files:\n                selected_files[character] = random.choice(available_files)\n                used_files.add(selected_files[character])\n            else:\n                # If ran out of voice, just reuse old one\n                selected_files[character] = random.choice(all_files)\n    \n    return selected_files\n\n# Function to convert text to speech for a character\ndef voice_character(character, text, page_number, bubble_number, selected_files, save_directory, name_format=\"page_{:03d}_panel_{:03d}_bubble_{:03d}{}\"):\n    speaker_wav = selected_files.get(character)\n    print(f\"Processing character '{character}' with text: {text}\")\n    if speaker_wav:\n        audio_output_filename = name_format.format(int(page_number), 0, bubble_number + 1, \".wav\")\n        output_filename = os.path.join(save_directory, audio_output_filename)\n        tts.tts_to_file(text=text, speaker_wav=speaker_wav, language=\"en\")\n        os.rename(\"output.wav\", output_filename)  # Rename the default output file\n        return output_filename\n    else:\n        raise ValueError(f\"Character '{character}' not found in voice mapping.\")\n\n# Function to process the transcript and create audio files\ndef text2speech(pages, selected_files, save_directory):\n    output_files = []\n\n    create_save_folder(save_directory)\n\n    for page in pages:\n        page_number = page[\"page\"]\n        for bubble_number, (character, dialogue) in enumerate(page[\"lines\"]):\n            try:\n                output = voice_character(character, dialogue, page_number, bubble_number, selected_files, save_directory)\n                output_files.append(output)\n            except ValueError as e:\n                print(e)\n    print(f\"Audio files have been saved to {save_directory}\")\n\n    return output_files\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Generate speech from a transcript using pre-recorded voice files.\")\n    parser.add_argument(\"-i\", \"--images_folder\", default=\"input/raw\", required=True, type=str, help=\"Directory containing manga images.\")\n    parser.add_argument(\"-v\", \"--voice_bank\", default=\"input/voice_bank\", type=str, help=\"Directory containing voice files.\")\n    parser.add_argument(\"-t\", \"--transcript\", default=\"output/transcript\", required=True, type=str, help=\"Path to the transcript text file.\")\n    parser.add_argument(\"-o\", \"--output\", required=True, type=str, help=\"Directory to save the generated audio files.\")\n\n    return parser.parse_args()\n\nif __name__ == \"__main__\":\n    # Parse command-line arguments\n    args = parse_args()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running on {device}\")\n    # Mock input to automatically respond with 'y'\n    with patch('builtins.input', return_value='y'):\n        tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n    number_of_digit_for_name = get_digit_number_for_name_format(args.images_folder)\n    name_format = generate_name_format(number_of_digit_for_name)\n\n    # Parse the transcript file\n    pages = parse_transcript(args.transcript)\n    characters = {line[0] for page in pages for line in page[\"lines\"]}\n\n    # Select voice files for characters\n    selected_files = select_voice_files_for_characters(characters, args.voice_bank)\n    os.makedirs(args.output, exist_ok=True)\n\n    output_files = text2speech(pages, selected_files, args.output)\n\n# HOW TO USE\n# !python /kaggle/working/manga_read_along/magi_functional/text_to_speech.py -i {raw_image_rename_path} -v {voice_bank_path} -t {transcript_file} -o {audio_path}\n'''\n\nwith open(\"/kaggle/working/magi_functional/text_to_speech.py\", \"w\") as file:\n    file.write(process_file)\n    print(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T20:45:44.776720Z","iopub.execute_input":"2025-01-01T20:45:44.777031Z","iopub.status.idle":"2025-01-01T20:45:44.784406Z","shell.execute_reply.started":"2025-01-01T20:45:44.777007Z","shell.execute_reply":"2025-01-01T20:45:44.783512Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Experienmental notebook","metadata":{}},{"cell_type":"markdown","source":"## Download kaggle data","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/Actor_*","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:35:35.411480Z","iopub.execute_input":"2024-11-11T19:35:35.411928Z","iopub.status.idle":"2024-11-11T19:35:36.718065Z","shell.execute_reply.started":"2024-11-11T19:35:35.411887Z","shell.execute_reply":"2024-11-11T19:35:36.716502Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"%cd /kaggle/working/\n\n# Step 1: Install Kaggle API if not already installed\n!pip install -q kaggle\n\n# # Step 2: Set up Kaggle authentication (move kaggle.json to ~/.kaggle/ directory)\n# mkdir -p ~/.kaggle\n# mv /path/to/kaggle.json ~/.kaggle/\nvoice_bank_path = \"/kaggle/working/manga_read_along/input/voice_bank\"\n!mkdir {voice_bank_path}\n%cd {voice_bank_path}\n\n# Step 3: Download the dataset\n!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n\n# Step 4: Unzip the dataset\n!unzip ravdess-emotional-speech-audio.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef get_voice_files(directory):\n    all_files = []\n    # Walk through the directory and subdirectories\n    for root, _, files in os.walk(directory):\n#         print(f\"Checking in directory: {root}\")  # Debug: Print the current directory being checked\n        for file in files:\n            # Check if the file is a .wav file (case insensitive)\n            if file.lower().endswith(\".wav\"):\n                all_files.append(os.path.join(root, file))\n#                 print(f\"Found file: {file}\")  # Debug: Print the file name found\n    return all_files\n\n# Get all .wav files in the voice_bank directory\nall_files = get_voice_files(voice_bank_path)\n\n# Print the list of all found files\nprint(f\"All found voice files: {all_files}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Magiv2","metadata":{}},{"cell_type":"markdown","source":"#### Create raw and character/names list (should apply rename first dumbass, TODO)","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\n\ndef create_chapter_pages_and_character_bank(manga_folder, character_folder):\n    # Create lists for chapter pages and character bank\n    chapter_pages = []\n    character_bank = {\n        \"images\": [],\n        \"names\": []\n    }\n\n#     Iterate through manga images to create chapter_pages\n    for image_file in os.listdir(manga_folder):\n        if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Extract the page number using regex\n            match = re.search(r'p(\\d+)', image_file)\n            if match:\n                page_number = int(match.group(1))  # Convert to integer for sorting\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n            else:\n                page_number = image_file.rsplit(\".\", 1)[0]\n                chapter_pages.append((page_number, image_file))  # Store as tuple (page_number, image_file)\n\n    # Sort chapter pages by page number\n    chapter_pages.sort(key=lambda x: x[0])\n    chapter_pages = [os.path.join(manga_folder, img[1]) for img in chapter_pages]  # Extract just the filenames after sorting\n\n    # Iterate through character images to create character bank\n    for char_image_file in os.listdir(character_folder):\n        if char_image_file.endswith(('.png', '.jpg', '.jpeg')):  # Check for image file extensions\n            # Split the filename to extract character name\n            char_name = char_image_file.split('_')[0]  # Get the part before the underscore\n            character_bank[\"images\"].append(os.path.join(character_folder, char_image_file))\n            character_bank[\"names\"].append(char_name)\n    return chapter_pages, character_bank\n\n# Define your folders\n# One Piece\n# manga_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/raw_manga\")\n# character_folder = Path(\"/kaggle/working/magi_functional/data_test/personal_data/One_Piece/character\")\n\n# Ruri Dragon\nmanga_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/raw\")\ncharacter_folder = Path(\"/kaggle/working/manga_read_along/magi_functional/data_test/personal_data/Ruri_Dragon/character\")\n\n# Get chapter pages and character bank\nchapter_pages_original, character_bank_original = create_chapter_pages_and_character_bank(manga_folder, character_folder)\n\nchapter_pages_test = chapter_pages_original\ncharacter_bank_test = character_bank_original\n\n# Print the results (for debugging)\nprint(\"Chapter Pages:\")\nprint(chapter_pages_test)\n\nprint(\"\\nCharacter Bank:\")\nprint(character_bank_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Process (OCR â†’ Transcript)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\ndef read_image(path_to_image):\n    with open(path_to_image, \"rb\") as file:\n        image = Image.open(file).convert(\"L\").convert(\"RGB\")\n        image = np.array(image)\n    return image\n\n\nchapter_pages = [read_image(x) for x in chapter_pages_test]\ncharacter_bank = character_bank_test.copy()\ncharacter_bank[\"images\"] = [read_image(x) for x in character_bank_test[\"images\"]]\n\nwith torch.no_grad():\n    per_page_results = magiv2.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)\n\nprint(\"Continue with next cell\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save transcript and json file","metadata":{}},{"cell_type":"code","source":"result_dir = \"/kaggle/working/result\"\njson_output_dir = f\"{result_dir}/json_results\"\nresult_image_output_dir = f\"{result_dir}/image_results\"\ntranscript_output_dir = f\"{result_dir}/transcript.txt\"\nos.makedirs(json_output_dir, exist_ok=True)  # Create the directory if it doesn't exist  \nos.makedirs(result_image_output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\ntranscript = []\nfor i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):\n    image_name_ext = os.path.basename(chapter_pages_test[i]) \n    # Split the image name and its extension\n    image_name, image_extension = os.path.splitext(image_name_ext)\n    \n#     model.visualise_single_image_prediction(image, page_result, os.path.join(result_image_output_dir, f\"{image_name}.png\")) # enable this if you want to see the result image included with all the annotation box\n    # Save page_result to JSON\n    json_file_path = os.path.join(json_output_dir, f\"{image_name}.json\")\n    with open(json_file_path, 'w') as json_file:\n        json.dump(page_result, json_file, indent=4)\n\n    speaker_name = {\n        text_idx: page_result[\"character_names\"][char_idx] for text_idx, char_idx in page_result[\"text_character_associations\"]\n    }\n    \n    transcript.append(f\"<page>{image_name}<endpage>\")\n    for j in range(len(page_result[\"ocr\"])):\n        if not page_result[\"is_essential_text\"][j]:\n            continue\n        name = speaker_name.get(j, \"unsure\") \n        transcript.append(f\"<name>{name}<endname>: {page_result['ocr'][j]}\")\nwith open(transcript_output_dir, \"w\") as fh:\n    for line in transcript:\n        fh.write(line + \"\\n\")\n\nprint(\"\\n\\nDone you WEEEEB!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Text to Speech","metadata":{}},{"cell_type":"code","source":"voice_mapping = {\n'other': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-01-02.wav\",\n'ruri': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_04/03-01-01-01-01-01-04.wav\",\n'teacher': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\",\n'mom': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_08/03-01-01-01-01-01-08.wav\",\n'unsure': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_10/03-01-01-01-01-01-10.wav\",\n'ukka': \"/kaggle/input/ravdess-emotional-speech-audio/Actor_12/03-01-01-01-01-01-12.wav\"\n}\n ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from magi_functional.utils.utils import rename_image_to_correct_format, get_digit_number_for_name_format, generate_name_format\n\nimages_folder = '/kaggle/working/manga_read_along/input/colorized'\n\nnumber_of_digit_for_name = get_digit_number_for_name_format(images_folder)\nname_format = generate_name_format(number_of_digit_for_name)\nprint(name_format)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to parse the transcript from a text file\ndef parse_transcript(transcript_path, line_end: int = None):\n    pages = []\n    current_page = None\n\n    with open(transcript_path, 'r') as file:\n        content = file.readlines()\n        \n    lines_to_process = content[:line_end] if line_end is not None else content\n    \n    for line in lines_to_process:\n        if line.startswith(\"<page>\"):\n            # Start a new page\n            if current_page is not None:\n                pages.append(current_page)\n            current_page = {\"page\": re.search(r'<page>(\\d+)<endpage>', line).group(1), \"lines\": []}      \n        elif line.startswith(\"<name>\") and current_page is not None:\n            # Extract character name and dialogue\n            match = re.match(r\"<name>([^<]+)<endname>:\\s*(.+)\", line)\n            if match:\n                character = match.group(1).lower()  # Lowercase for consistency\n                dialogue = match.group(2)\n                current_page[\"lines\"].append((character, dialogue))\n\n    # Add the last page if it exists\n    if current_page is not None:\n        pages.append(current_page)\n        \n    return pages\n\n# Function to get all voice files from the specified directory\ndef get_voice_files(directory):\n    all_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".wav\"):\n                all_files.append(os.path.join(root, file))\n    return all_files\n\n# Function to filter voice files into male and female categories\ndef filter_voice_files(files):\n    male_files = []\n    female_files = []\n    \n    for file in files:\n        # Extract the actor number from the filename\n        match = re.search(r\"(\\d{2})\\.wav$\", file)\n        if match:\n            actor_number = int(match.group(1))\n            if actor_number % 2 == 0:  # Even numbers are female\n                female_files.append(file)\n            else:  # Odd numbers are male\n                male_files.append(file)\n                \n    return male_files, female_files\n\n# Function to randomly select voice files for characters\ndef select_voice_files_for_characters(characters):\n    all_files = get_voice_files(voice_bank)\n    male_files, female_files = filter_voice_files(all_files)\n\n    selected_files = {}\n    used_files = set()  # To keep track of used voice files\n\n    for character in characters:\n        if character in male_characters:\n            # Select a random male voice that hasn't been used\n            available_male_files = list(set(male_files) - used_files)\n            if available_male_files:\n                selected_files[character] = random.choice(available_male_files)\n                used_files.add(selected_files[character])\n        else:\n            # Select a random female voice that hasn't been used, if available\n            available_female_files = list(set(female_files) - used_files)\n            if available_female_files:\n                selected_files[character] = random.choice(available_female_files)\n                used_files.add(selected_files[character])\n            else:\n                # Select any remaining file for other characters, ensuring it's not already used\n                available_files = list(set(all_files) - used_files)\n                if available_files:\n                    selected_files[character] = random.choice(available_files)\n                    used_files.add(selected_files[character])\n    \n    return selected_files\n\n# Function to convert text to speech for a character\ndef voice_character(character, text, page_number, bubble_number, selected_files, save_directory):\n    speaker_wav = selected_files.get(character)\n\n    if speaker_wav:\n        audio_output_filename = name_format.format(int(page_number), 0, bubble_number+1, \".wav\")\n        output_filename = os.path.join(save_directory, audio_output_filename)\n        output = tts.tts_to_file(text=text, speaker_wav=speaker_wav, language=\"en\")\n        os.rename(\"output.wav\", output_filename)  # Rename the default output file\n        return output_filename\n    else:\n        raise ValueError(f\"Character '{character}' not found in voice mapping.\")\n\n# Function to process the transcript and create audio files\ndef text2speech(pages, selected_files, save_directory):\n    output_files = []\n    time_file = []\n    \n    for page in pages: \n        page_number = page[\"page\"]\n        for bubble_number, (character, dialogue) in enumerate(page[\"lines\"]):\n            try:\n                output = voice_character(character, dialogue, page_number, bubble_number, selected_files, save_directory)\n                output_files.append(output)\n            except ValueError as e:\n                print(e)\n\n    return output_files","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the directory where the RAVDESS files are located\nvoice_bank = \"/kaggle/input/ravdess-emotional-speech-audio/\"\n# Define male characters for the check\nmale_characters = ['teacher']\ntranscript_file = f\"{transcript_path}/transcript.txt\"\n\nsave_directory = audio_path\n\nif os.path.exists(save_directory):\n    shutil.rmtree(save_directory)\nos.makedirs(save_directory, exist_ok=True)\n\n# Get unique characters from the transcript\npages = parse_transcript(transcript_file, line_end = 20) # delete line_end to get audio of all page\ncharacters = {line[0] for page in pages for line in page[\"lines\"]}  # Get unique characters\n\n# Select voice files for characters\nselected_files = select_voice_files_for_characters(characters)\n\n# # Call the function to process the transcript and generate audio files\noutput_files = text2speech(pages, selected_files, save_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}